{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A(nother) hands-on intro to GP regression\n",
    "\n",
    "**Original Author: Dan Foreman-Mackey** (foreman.mackey@gmail.com)\n",
    "\n",
    "**A(nother) Author: Muchen Sun** (sunmch15@gmail.com)\n",
    "\n",
    "~~~\n",
    "Note: \n",
    "This tutorial is heavily based on Dan Foreman-Mackey's original amazing hands-on GP tutorial, which literally helps me enter the world of GP. After finishing the tutorial, as a reader/user myself, I think this tutorial might be even more helpful if I could include more details, so here we are.\n",
    "\n",
    "Compared with original version, here I added more explanation about multi-variate Gaussian distribution, the role of covariance matrix, and exposed more details of regression with GP.\n",
    "~~~\n",
    "\n",
    "In this tutorial, we'll work through an example problem in Gaussian process (GP) regression using Python.\n",
    "The goal of the tutorial is to give you a qualitative sense of how GP regression works and a rough idea of how you might implement it in a real world problem.\n",
    "\n",
    "This is designed to be open-ended and the last sections should be instructive even if you're already familiar with the implementation of GPs.\n",
    "I've chosen to use Python because it is my programming language of choice and because it is commonly used in astronomy, but if you don't have experience coding in Python I've tried to include a lot of skeleton code so you should still be able to make progress without getting too caught up with Python's syntax.\n",
    "Also, if you ever find yourself writing a `for` loop then something is wrong; Python is an interpreted language and, as a result, looping is *very* slow.\n",
    "This entire worksheet can be done without looping.\n",
    "\n",
    "**Remember: The best reference for all things GPs is [Rasmussen & Williams](http://www.gaussianprocess.org/gpml/).**\n",
    "\n",
    "*And also: Richard Turner's [lecture](https://www.youtube.com/watch?v=92-98SYOdlY) is also a very good start point (especially when you got stuck at the book like me).*\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First let's set up this notebook by importing everything we'll need and choose some settings so that the plots look better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPython.matplotlib.backend = \"retina\"\n",
    "\n",
    "# Start by importing the specific code for this project.\n",
    "import gp\n",
    "\n",
    "# And then the standard numerical Python modules.\n",
    "import emcee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finally set up the plotting to look a little nicer.\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Automatically reload library\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Hello, Gaussian!\n",
    "\n",
    "Before we \"officially\" enter the world of Gaussian process, let's first review some concepts related to Gaussian distribution in this section.\n",
    "\n",
    "### 0.1 - One-Dimensional Gaussian Distribution\n",
    "\n",
    "Gaussian distribution can be represented by a probability density function(PDF), which tells us, how **likely** we may observe a sample under this distribution. For a one-dimensional distribution, the PDF is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x | \\mu, \\sigma) & = f(x, \\mu, \\sigma) \\\\\n",
    "                   & = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here the **mean** $\\mu$ indicates where the likelihood is maximum, and the **standard deviation** represents how \"flat\" the curve it.\n",
    "\n",
    "Let's implement the PDF and see how things go: finish the `pdf_1D` function below, and set it as input to `setup_gaussian_distr_1D` function to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_1D(x, mu, sig):\n",
    "    \"\"\"\n",
    "    The univariate Gaussian likelihood of vector ``x`` given a mean \n",
    "    ``mean`` and a standard deviation ``stddev``.\n",
    "    \n",
    "    :param x: ``(N,)``   Points to bw evaluated at.\n",
    "    :param mu: ``float`` Mean of the univariate Gaussian likelihood function.\n",
    "    :param sig: ``float`` Standard Deviation of the univariate Gaussian \n",
    "                             likelihood function.\n",
    "    \n",
    "    :returns ll: ``float`` The Gaussian likelihood. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement the one-dimensional \n",
    "    # Gaussian likelihood function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_gaussian_distr_1D(pdf_1D)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 - Two-Dimensional Gaussian Distribution\n",
    "\n",
    "Now let's think about a two-dimensional Gaussian distribution, the mean becomes a 2-vector, and the variable that describe the \"flatness\" is now a 2-by-2 matrix $\\Sigma$, which we call **covariance matrix**. The PDF can be represented as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x | \\mu, \\Sigma) & = f(x, \\mu, \\Sigma) \\\\\n",
    "                   & = \\sqrt{det(2\\pi\\Sigma)} \\exp\\left(-\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The diagonal elements in covariance matrix describes the uncertainty of each element in the vector, but that's only part of the story -- the off-diagonal elements describes how different elements in the vector are correlated to each other.\n",
    "\n",
    "Let's implement this PDF below in `pdf_2D`, and set it as input to `setup_gaussian_distr_2D` to visualize the likelihood distribution. The mean is fixed at the origin, and diagonal elements in covariance matrix are both fixed at 1.0, slide the `off_diagonal` value (note that covariance matrix is always symmetric), and see how the PDF changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_2D(x, mu, sig):\n",
    "    \"\"\"\n",
    "    The Gaussian likelihood of vector ``x`` given a mean \n",
    "    ``mean`` and a standard deviation ``stddev``.\n",
    "    \n",
    "    :param x: ``(2,)``   Point to be evaluated at.\n",
    "    :param mu: ``(2,)`` Mean of the Gaussian likelihood function.\n",
    "    :param sig: ``(2,2)`` Covariance matrix of the Gaussian likelihood function.\n",
    "    \n",
    "    :returns ll: ``float`` The Gaussian likelihood. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement the two-dimensional \n",
    "    # Gaussian likelihood function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_gaussian_distr_2D(pdf_2D)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 - Marginal, Conditional and Joint Distribution\n",
    "\n",
    "To understand what it means by the \"correlation between two elements\", we first introduce the concepts of **marginal** and **conditional distribution**.\n",
    "\n",
    "We begin by re-write the two-dimensional Gaussian distribution in a more detailed manner (and with a new shorthand):\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\\mu_x \\\\ \\mu_y\\end{bmatrix}, \\Sigma = \\begin{bmatrix}A & C \\\\ C^\\top & B\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, for a 2-vector that's under this distribution, we can represent it using the shorthand:\n",
    "$$\n",
    "\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\sim \\mathcal{N}\\left(\\begin{bmatrix}\\mu_x \\\\ \\mu_y\\end{bmatrix}, \\begin{bmatrix}A & C \\\\ C^\\top & B\\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "**Marginal distribution** indicates a subset of the original distribution, and a nice property of Gaussian distribution is, *the marginal distribution of Gaussian distribution is still a Gaussian distribution*. For example, if I only care about the distribution of $x$, then the marginal distribution of $x$ can be easily represented by extracting the corresponding elements from original mean and covariance:\n",
    "$$\n",
    "x \\sim \\mathcal{N}\\left(\\mu_x, A\\right)\n",
    "$$\n",
    "\n",
    "But knowing $y$ might help us to better understand $x$, so we're also interested in the distribution of $x$ given the value of $y$, we call it **conditional distribution**, which (not that surprisingly) is also a Gaussian distribution:\n",
    "$$\n",
    "x|y \\sim \\mathcal{N}\\left( \\mu_x+CB^{-1}(y-\\mu_y), A - CB^{-1}C^\\top \\right)\n",
    "$$\n",
    "Note that here we use the matrix form, this is for the general formulation of higher dimensional varialbles, in our two-dimenional example, $A, B, C$ are scalars. The above equation for conditional distribution will be very important later for understanding Gaussian process.\n",
    "\n",
    "Now, implement the equation for conditional distribution in the function `conditional_pdf` below, then visualize the result using `setup_gaussian_distr_2D_conditional`. Try different values for $y$ and off-diagonal elements, you will find that, when off-diagonal elements are positive, the mean of $x$'s conditional distribution with increase with $y$. This means $x$ is positively correlated with $y$ -- when $y$ increases, $x$ will *likely* increase, too! When off-diagonal elements are zero, moving $y$ doesn't influence $x$'s conditional distribution, this means knowing $y$ tells us nothing about $x$.\n",
    "\n",
    "*(hint: the conditional distribution here is just a one-dimensional Gaussian distribution, so feel free to use the function you previosly implemented.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_pdf(x, y, mu, sig):\n",
    "    \"\"\"\n",
    "    The conditional likelihood of ``x`` given a condition of ``y``,\n",
    "    mean ``mean`` and a standard deviation ``stddev``.\n",
    "    \n",
    "    :param x: ``(N,)``   Point to be evaluated at.\n",
    "    :param y: ``(1,)``   Point to be conditioned at.\n",
    "    :param mu: ``(2,)`` Mean of the Gaussian likelihood function.\n",
    "    :param sig: ``(2,2)`` Covariance matrix of the Gaussian likelihood function.\n",
    "    \n",
    "    :returns ll: ``float`` The Gaussian likelihood. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement the conditional distribution\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_gaussian_distr_2D_conditional(pdf_2D, conditional_pdf)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, sometimes we have two independent Gaussian distributions, and we want to know the likelihood of obtaining two samples at the same time, this is the **joint distribution** of two *independent* Gaussian distributions, which (again!) remains a Gaussian (the assumption of independence is important here, otherwise things will be much more complicated). In this case, the joint distribution is simply the product of two Gaussians:\n",
    "$$\n",
    "\\mathcal{N}(x|a,A) \\mathcal{N}(x|b,B) = Z^{-1} \\mathcal{N}\\left( (A^{-1}+B^{-1})^{-1} (A^{-1}a+B^{-1}b), (A^{-1}+B^{-1})^{-1} \\right)\n",
    "$$\n",
    "Here $Z$ is the normalizer.\n",
    "\n",
    "Finally, with everything ready, let's start to think about the problem of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 – Fitting a \"transit\" with \"correlated noise\"\n",
    "\n",
    "In this section, we'll work through how to fit a simplified transit model to data in the presence of \"correlated noise\".\n",
    "I think that it's important to clarify what I mean by \"noise\" here because it isn't necessarily what you immediately expect.\n",
    "When I say noise, I'm referring to *everything that contributes to generating the data that isn't an exoplanet transit*.\n",
    "In the context of transit fitting, this means that everything that the spacecraft is doing (pointing and temperature variations, etc.) and everything that the star is doing (rotation, asteroseismic oscillations, granulation, etc.) is \"noise\".\n",
    "As you can imagine, in many cases this noise might actually be the thing that you're trying to measure and that's fine – I spend most of my time measuring noise!\n",
    "Therefore, more formally, \"noise model\" refers to anything that we're modeling as **stochastic** and the \"mean model\" refers to anything that we're modeling as **deterministic**.\n",
    "For the purposes of this tutorial, we're not going to interpret the stochastic component of the signal and, instead, focus on studying the mean model, but that is not a limitation of GP modeling in general.\n",
    "\n",
    "### 1.1 – The \"mean model\"\n",
    "\n",
    "In this tutorial, we'll be fitting a very simple \"transit\" model with three parameters (depth, duration, and transit time) to a simulated dataset.\n",
    "This model is implemented in the `gp` module that was included with this notebook and here's an example of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-5, 5, 5000)\n",
    "model = gp.SillyTransitModel(log_depth=np.log(200.0), log_duration=np.log(0.5), time=0.0)\n",
    "plt.plot(t, model.get_value(t))\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"flux [ppm]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This obviously isn't meant to be realistic, but it's a good enough toy problem for today.\n",
    "This model has a few nice features that might come in handy later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"parameter_dict:\")\n",
    "print(model.get_parameter_dict())\n",
    "\n",
    "print(\"\\nparameter_vector:\")\n",
    "print(model.get_parameter_vector())\n",
    "\n",
    "print(\"\\nyou can freeze and thaw parameters...\")\n",
    "print(\"the depth, for example, is now fixed to the current value:\")\n",
    "model.freeze_parameter(\"log_depth\")\n",
    "print(model.get_parameter_dict())\n",
    "\n",
    "print(\"\\nand 'thawed':\")\n",
    "model.thaw_parameter(\"log_depth\")\n",
    "print(model.get_parameter_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 – Fitting the usual way\n",
    "\n",
    "I've included some code to simulate a dataset with a \"transit\" given by our silly model.\n",
    "Here's how you get it and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, yerr = gp.data.generate_dataset()\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\")\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"flux [ppm]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that there's some white noise (the error bars show that part) but that something else is going on.\n",
    "There should be some sort of \"trend\" on top of which the transit is superimposed.\n",
    "This might have been caused by spots rotating into and out of view, motion of the spacecraft, or something else.\n",
    "As I mentioned at the top, we won't worry too much about interpreting these trends.\n",
    "\n",
    "Now, let's start by fitting this data the usual way: ignoring the correlated noise.\n",
    "(I recognize that I'm being a bit unfair here and point out that most transit fitting results also fit out some parametric form for the trend, but this demo gets today's point across so bear with me...)\n",
    "\n",
    "The usual likelihood function that we can derive assuming independent Gaussian noise is (*recall our one-dimensional Gaussian likelihood in Section 0*):\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathrm{data}\\,|\\,\\mathrm{model}) = -\\frac{1}{2}\\sum_{n=1}^N\\left[\\frac{[y_n - m(t_n)]^2}{{\\sigma_n}^2} + \\ln(2\\,\\pi\\,{\\sigma_n}^2)\\right ] = -\\frac{1}{2}\\,\\chi^2 + \\mathrm{constant}\n",
    "$$\n",
    "\n",
    "where $m(t)$ is the transit model and we might maximize this (or, equivalently, minimize $\\chi^2$) to find the \"best-fit\" parameters.\n",
    "Instead, we'll apply improper uniform priors (*Disclaimer: don't think that I condone this behavior*) and sample the posterior probability density for the parameters of the model using [emcee](http://dan.iel.fm/emcee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_log_prob(params, model, t, y, yerr):\n",
    "    model.set_parameter_vector(params)\n",
    "    resid = y - model.get_value(t)\n",
    "    return -0.5 * np.sum((resid/yerr)**2)\n",
    "\n",
    "ndim = 3\n",
    "nwalkers = 16\n",
    "pos = 1e-4*np.random.randn(nwalkers, ndim)\n",
    "pos[:, 0] += 5\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, simple_log_prob,\n",
    "                                args=(model, t, y, yerr))\n",
    "pos, _, _ = sampler.run_mcmc(pos, 200)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, 2000);\n",
    "\n",
    "print('true parameters:\\n', gp.data.true_parameters)\n",
    "\n",
    "gp.corner(sampler.flatchain, truths=gp.data.true_parameters,\n",
    "          labels=[\"ln(depth)\", \"ln(duration)\", \"transit time\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, you should see the posterior constraints on the parameters compared to the true values that were used to simulate the data shown as blue lines.\n",
    "If everything went as planned, this should be your feeling about the result:\n",
    "\n",
    "<div style=\"font-size:200%;padding:20px 0 0\">😕</div>\n",
    "\n",
    "This didn't work because we ignored the correlated noise! In the above likelihood function, we only consider the marginal likelihood of each observation, but the missing conditional likelihood, or the correlation between the observations plays an important role here.\n",
    "\n",
    "How to fix this? Let's try to consider the whole covariance matrix instead of only diagonal elements. And this brings us to...\n",
    "\n",
    "### 1.3 – Building a Gaussian process model\n",
    "\n",
    "Now let’s implement the code for a Gaussian process.\n",
    "The main thing that you need to do in this section is write the ln-likelihood function.\n",
    "What you need to do is convert the following mathematical equation (recall the two-dimensional Gaussian likelihood in Section 0) to Python:\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathrm{data}\\,|\\,\\mathrm{model}) = \\ln L = -\\frac{1}{2}\\,\\boldsymbol{r}^\\mathrm{T}\\,K^{-1}\\,\\boldsymbol{r} - \\frac{1}{2}\\,\\ln \\det \\boldsymbol{K} - C\n",
    "$$\n",
    "\n",
    "where $C = N\\,\\ln(2\\,\\pi)/2$ is an irrelevant (for our purposes) constant, $\\boldsymbol{r}$ is the residual vector (the difference between the measured $y$ and the model predictions for $y$), and $K$ is the covariance matrix describing the measurement uncertainties and the correlations between observations.\n",
    "\n",
    "In the following cell, you'll implement a Python function that takes as input a residual vector `r` and a covariance matrix `K` and returns the right-hand side of the above equation.\n",
    "In pseudocode, the function will look something like:\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Input:** Residual vector $\\boldsymbol{r}$; covariance matrix $K$<br>\n",
    "**Output:** The ln-likelihood (up to a constant)\n",
    "1. $\\boldsymbol{\\alpha} = \\mathtt{np.linalg.solve}({K},\\,\\boldsymbol{r})$\n",
    "2. $(s,\\,d) = \\mathtt{np.linalg.slogdet}(\\boldsymbol{K})$\n",
    "3. **return** $-0.5\\,\\left(\\boldsymbol{r}\\cdot\\boldsymbol{\\alpha} + d\\right)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "When implementing this function, the two numpy (remember that we imported `numpy` as `np` above) methods used in the pseudocode ([`np.linalg.solve`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html) and [`np.linalg.slogdet`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html)) will come in handy.\n",
    "The `solve` method is useful because you should *never* invert a matrix (it is numerically unstable).\n",
    "Instead, you should always use `solve` if you want to find $\\boldsymbol{x}$ in the equation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} = {K}^{-1}\\,\\boldsymbol{r} \\to \\boldsymbol{r} = {K}\\,\\boldsymbol{x} \\quad.\n",
    "$$\n",
    "\n",
    "The `slogdet` function is useful because it computes the ln-determinant of a matrix using a numerically stable method.\n",
    "Determinants of physical matrices tend to be *very* tiny and computing them naively can result in numerical underflow.\n",
    "Sticking to the ln-determinant will make you less suceptible to this problem.\n",
    "\n",
    "Finally, if you want to take the dot product of two vectors or matrices, you should use the [`np.dot`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) function.\n",
    "\n",
    "(*If you're already an expert, you can actually speed this code up substantially using the [cho_factor](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.linalg.cho_factor.html) and [cho_solve](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.linalg.cho_solve.html) functions from scipy and figuring out how to compute the determinant of a matrix using its [Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition). Follow the links to documentation for `cho_factor` to see what, exactly, it returns. You can extract the diagonal elements from a matrix using the [`np.diag`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html) function.\n",
    "Other functions you may want to use include `np.sum` and `np.log`.*) \n",
    "\n",
    "OK. Go for it! Bonus points if your implementation fits in a Tweet (don't forget to include the #KeplerSciCon hashtag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(r, K):\n",
    "    \"\"\"\n",
    "    The multivariate Gaussian ln-likelihood (up to a constant) for the\n",
    "    vector ``r`` given a covariance matrix ``K``.\n",
    "    \n",
    "    :param r: ``(N,)``   The residual vector with ``N`` points.\n",
    "    :param K: ``(N, N)`` The square (``N x N``) covariance matrix.\n",
    "    \n",
    "    :returns lnlike: ``float`` The Gaussian ln-likelihood. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement the Gaussian process\n",
    "    # ln-likelihood here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're happy with your implementation, execute the following cell.\n",
    "If your implementation is correct, you'll see a smiley face (☺︎) otherwise, it will throw an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.utils.test_log_like(log_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see a happy face when you executed the above line?\n",
    "Edit your `log_like` function until you do and then let's move on to implementing a specific model.\n",
    "\n",
    "### 1.4 – The covariance matrix/kernel function\n",
    "\n",
    "Now that we have the likelihood function implemented, we need to choose a specific model of the covariance matrix $K$.\n",
    "Each element of the matrix $K_{ij}$, will be given by\n",
    "\n",
    "$$\n",
    "K_{ij} = k_{\\boldsymbol{\\alpha}}(x_i,\\,x_j)\n",
    "$$\n",
    "\n",
    "where we get to choose the \"kernel function\" $k_{\\boldsymbol{\\alpha}}$ (parameterized by ${\\boldsymbol{\\alpha}}$).\n",
    "For now, let's implement the *exponential-squared kernel*:\n",
    "\n",
    "$$\n",
    "k_{\\boldsymbol{\\alpha}}(r_{ij}) = a^2 \\, \\exp \\left ( -\\frac{{\\Delta x_{ij}}^2}{2\\,l^2} \\right )\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\alpha} = (a,\\,l)$ and $\\Delta x_{ij} = |x_i - x_j|$.\n",
    "\n",
    "You need to implement this function below and it needs to work with a `numpy` array `dx` as input (use the `np.exp` function for exponentiation).\n",
    "Note that `alpha` will be a list with two elements: `alpha[0] = amp` and `alpha[1] = ell`; Python arrays are zero-indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expsq_kernel(alpha, dx):\n",
    "    \"\"\"\n",
    "    The exponential-squared kernel function. The difference matrix\n",
    "    can be an arbitrarily shaped numpy array so make sure that you\n",
    "    use functions like ``numpy.exp`` for exponentiation.\n",
    "    \n",
    "    :param alpha: ``(2,)`` The parameter vector ``(amp, ell)``.\n",
    "    :param dx: ``numpy.array`` The difference matrix. This can be\n",
    "        a numpy array with arbitrary shape.\n",
    "    \n",
    "    :returns K: The kernel matrix (should be the same shape as the\n",
    "        input ``dx``). \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement your kernel function\n",
    "    # there.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're happy with your implementation of `expsq_kernel`, try executing the following cell to test your code.\n",
    "Again, if you've implemented it correctly, you'll see a smiley face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.utils.test_kernel(expsq_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 – Qualitative effects of kernel (hyper)parameters\n",
    "\n",
    "Now that we've implemented our kernel function, let's try to get a qualitative sense of what the hyperparameters do.\n",
    "\n",
    "**Note:** *another good resource for trying this out is [gp.js](http://dan.iel.fm/gp.js/)*.\n",
    "\n",
    "To start, we'll generate samples from the likelihood function that we implemented.\n",
    "Remember that a likelihood is a probability distribution over *data* so, conditioned on specific hyperparameters, we can sample synthetic datasets from the likelihood.\n",
    "For a Gaussian process, this probability function is a multivariate Gaussian where the covariance matrix is given by our kernel function.\n",
    "\n",
    "In the following cell, we're using some IPython magic to create an interactive figure that displays 6 samples $\\boldsymbol{y}_k$ from the likelihood for variable values of the hyperparameters.\n",
    "Try executing the cell and then moving the sliders to get a feel for how the samples change with $a$ (`amp`) and $l$ (`ell`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.interactive.setup_likelihood_sampler(expsq_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional distribution is even more interesting.\n",
    "That is, given the data that you have observed and a specific set of parameters, what is your prediction at other points $\\boldsymbol{x}_\\star$?\n",
    "The math can be found in Chapter 2 of [R&W](http://www.gaussianprocess.org/gpml/), you can just execute the following cell and play around with the parameters to see how the predictions change with all the parameter choices.\n",
    "\n",
    "In the figure, the \"noiseless\" transit function is shown as a dotted red line and the 6 samples from the conditional distribution are shown as black lines.\n",
    "The ln-likelihood is shown in the title of the figure.\n",
    "We'll use the results of your experiments to initialize the MCMC sampler in a bit so try to get a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional distribution is even more interesting. That is, given the data that you have observed, what is your prediction at other points? Let's denote the observations as vector $f\\in\\mathbb{R}^n$, and the points to be predicted as $f_*\\in\\mathbb{R}^{n_*}$. Combining the two vectors into a single one, and assume they're under the same Gaussian distribution (here we simply assume mean to be zero):\n",
    "$$\n",
    "\\begin{bmatrix}f\\\\f_*\\end{bmatrix} \\sim \\mathcal{N}\\left( 0, \\begin{bmatrix} K(X,X) & K(X,X_*) \\\\ K(X_*,X) & K(X_*,X_*) \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Here $K(X,X)\\in\\mathbb{R}^{n\\times n}, K(X,X_*)\\in\\mathbb{R}^{n\\times n_*}, K(X,X_*)=K(X,X_*)^\\top, K(X_*,X_*)\\in\\mathbb{R}^{n_*\\times n_*}$ are similar to $A, B, C$ in the two-dimensional Gaussian likelihood in Section 0, the difference is here we just wrote a kernel to describe this larger covariance matrix.\n",
    "\n",
    "Now, to perform regression/prediction, we're interested in the distribution of $f_*$, conditioned on the observations $f$, following the equation in Section 0, we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_* | f \\sim \\mathcal{N}\\left(K(X_*,X) [K(X,X)+\\sigma_n^2 I]^{-1} f, \\\\ \n",
    "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad K(X_*,X_*)-K(X,X_*)^\\top[K(X,X)+\\sigma_n^2 I]^{-1}K(X,X_*)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The only difference here is $K(X,X)^{-1}$ is replaced by $[K(X,X)+\\sigma_n^2 I]^{-1}$, this is because here we assume our observations are noisy, and $\\sigma_n$ is the standard deviation for the independent observation noise (which is captured by the errorbar in the plot). If we take observation noise as zero, then the equation above is exactly the same as Section 0.\n",
    "\n",
    "A reference pseudocode is shown below, and again, for the purpose of numerical stability, here we will use Cholesky factorization. The full algorithm for Gaussian process regression can be found in Chapter 2 of [R&W](http://www.gaussianprocess.org/gpml/), the only missing part here is the construction of kernel, but given the kernel function this should be straightforward. More details about the implementation can found at `gp/interactive.py`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Input:** observation $y$, covariance $K(X,X)$, covariance $K(X,X_*)$, covariance $K(X_*,X_*)$ <br>\n",
    "**Output:** mean $\\mu_*$ and covariance $\\Sigma_*$ for the distribution of prediction, log marginal likelihood of observations $f$ under the predicted distribution\n",
    "1. $L$ = scipy.linalg.cho_factor(K(X,X))\n",
    "2. $\\alpha$ = scipy.linalg.cho_solve(L, y)\n",
    "3. $\\mu_*$ = $K(x,x_*) \\cdot \\alpha$\n",
    "4. $\\Sigma_*$ = $K(x_*,x_*) - K(x,x_*) \\cdot$ np.linalg.solve(K(x,x)$\\cdot K(x,x_*)^\\top$)\n",
    "5. $ll$ = -0.5 $f^\\top \\alpha - \\sum_i \n",
    "\\log(L_{ii})$ - $\\frac{n}{2} \\log(2\\pi)$\n",
    "6. **return: $\\mu, \\Sigma, ll$**\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now, implement the function `kernel_training_zero_mean` and feed it to `setup_conditional_sampler_zero_mean` to generate six sample predictions from the resulting Gaussian distribution. Tune the kernel parameters and see how they change the sample, and try to find a set of parameters that optimizes the log marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_training_zero_mean(y, Kxx, Kxs, Kss):\n",
    "    \"\"\"\n",
    "    Train GP kernel given a set of observations (assuming zero mean)\n",
    "    \n",
    "    :param y, : ``(n,)`` vector of observations.\n",
    "    :param Kxx: ``(n,n)`` covariance matrix K(x,x)\n",
    "    :param Kxs: ``(n,n*)`` covariance matrix K(x,x*)\n",
    "    :param Kss: ``(n*,n*)`` covariance matrix K(x*,x*)\n",
    "    \n",
    "    :returns mu: mean of the prediction/conditional distributions\n",
    "    :returns cov: covariance of the prediction/conditional distribution\n",
    "    :returns ll: log likelihood of the given observations y under the \n",
    "                    prediction/conditional distribution\n",
    "    \"\"\"\n",
    "    from scipy.linalg import cho_factor, cho_solve\n",
    "    # Erase the following line and implement your function for computing\n",
    "    # the prediction distribution based on given observations and kernel\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_conditional_sampler_zero_mean(t, y, yerr, expsq_kernel, kernel_training_zero_mean)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may have some prior knowledge about the mean function. In this case, it might be better to indicate a certain mean instead of using the zero-mean function. Modify the kernel training function you just finished with a given non-zero mean function. Again, try to find a set of parameters that optimizes marginal likelihood.\n",
    "\n",
    "*(hint: only one tiny modification is needed!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_training_nonzero_mean(y, model_y, model_ys, Kxx, Kxs, Kss):\n",
    "    \"\"\"\n",
    "    Train GP kernel given a set of observations (with non-zero mean)\n",
    "    \n",
    "    :param y, : ``(n,)`` vector of observations.\n",
    "    :param model_y, : ``(n,)`` evaluation of non-zero mean function at\n",
    "                                observation points\n",
    "    :param model_ys, : ``(n*,)`` evaluation of non-zero mean function at\n",
    "                                predictioin points    \n",
    "    :param Kxx: ``(n,n)`` covariance matrix K(x,x)\n",
    "    :param Kxs: ``(n,n*)`` covariance matrix K(x,x*)\n",
    "    :param Kss: ``(n*,n*)`` covariance matrix K(x*,x*)\n",
    "    \n",
    "    :returns mu: mean of the prediction/conditional distributions\n",
    "    :returns cov: covariance of the prediction/conditional distribution\n",
    "    :returns ll: log likelihood of the given observations y under the \n",
    "                    prediction/conditional distribution\n",
    "    \"\"\"\n",
    "    # Erase the following line and implement your function for computing\n",
    "    # the prediction distribution based on given observations and kernel\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_conditional_sampler_nonzero_mean(t, y, yerr, expsq_kernel, kernel_training_nonzero_mean)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've found an initial guess that you're happy with, let's run some MCMC and see what our results look like. In this fit, we're going to sample for the parameters of the transit and the parameters of the GP (these ones are often called \"hyperparameters\") simultaneously. This means that any constraints that we get on the transit parameters will have the uncertainty in the noise model taken into account. Be warned, this MCMC will take a little longer to run than the last one because the GP model is a little bit more expensive to compute (we'll come back to this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gp_log_prob(params, log_like_fn, kernel_fn, mean_model, t, y, yerr):\n",
    "    if np.any(params < -10.) or np.any(params > 10.):\n",
    "        return -np.inf\n",
    "    \n",
    "    k = mean_model.vector_size\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    K = kernel_fn(np.exp(params[k:]), t[:, None] - t[None, :])\n",
    "    K[np.diag_indices_from(K)] += yerr**2\n",
    "    \n",
    "    # Compute the residual vector\n",
    "    mean_model.set_parameter_vector(params[:k])\n",
    "    resid = y - model.get_value(t)\n",
    "    \n",
    "    # Compute the log likelihood\n",
    "    return log_like_fn(resid, K)\n",
    "\n",
    "ndim = 5\n",
    "nwalkers = 16\n",
    "pos = [np.log(np.abs(widget.kwargs[k])) for k in [\"depth\", \"duration\", \"amp\", \"ell\"]]\n",
    "pos = np.array(pos[:2] + [widget.kwargs[\"time\"]] + pos[2:]) \n",
    "pos = pos + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, gp_log_prob,\n",
    "                                args=(log_like, expsq_kernel, model, t, y, yerr))\n",
    "pos, _, _ = sampler.run_mcmc(pos, 200)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, 2000);\n",
    "\n",
    "gp.corner(sampler.flatchain, truths=np.append(gp.data.true_parameters, [None, None]),\n",
    "          labels=[\"ln(depth)\", \"ln(duration)\", \"transit time\", \"ln(amp)\", \"ln(ell)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went as planned, you should have found a much more acceptable constraint on the parameters.\n",
    "You'll notice that the GP parameter panels don't have blue lines on them.\n",
    "This is because we don't know the \"true\" noise model; we're only using the GP as an *effective* model.\n",
    "\n",
    "## 2 - Optimize Hyperparameters For Maximal Likelihood\n",
    "\n",
    "Until now, the way we found a \"good\" set of (hyper)parameters is through manually trying, an alternative is to optimize over the paramters. In order to do so, there must be an objective function, and one choice for the objective the marginal likelihood of observations under the predicted GP posterior distribution, and in this section, we will implement this.\n",
    "\n",
    "Since, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_log_prob(params, log_like_fn, kernel_fn, mean_model, t, y, yerr):\n",
    "    if np.any(params < -10.) or np.any(params > 10.):\n",
    "        return -np.inf\n",
    "    \n",
    "    k = mean_model.vector_size\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    K = kernel_fn(np.exp(params[k:]), t[:, None] - t[None, :])\n",
    "    K[np.diag_indices_from(K)] += yerr**2\n",
    "    \n",
    "    # Compute the residual vector\n",
    "    mean_model.set_parameter_vector(params[:k])\n",
    "    resid = y - model.get_value(t)\n",
    "    \n",
    "    # Compute the log likelihood\n",
    "    return log_like_fn(resid, K)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# this is the intial guess from the sliders above\n",
    "pos = [np.log(np.abs(widget.kwargs[k])) for k in [\"depth\", \"duration\", \"amp\", \"ell\"]]\n",
    "pos = np.array(pos[:2] + [widget.kwargs[\"time\"]] + pos[2:])\n",
    "\n",
    "# since we use `minimize`, we need to use the negative likelihood as objective\n",
    "objective = lambda pos: -gp_log_prob(pos, log_like, expsq_kernel, model, t, y, yerr)\n",
    "\n",
    "# minimize the objective / maximize likelihood\n",
    "result = minimize(objective, pos)\n",
    "print('maximal likelihood: ', gp_log_prob(result.x, log_like, expsq_kernel, model, t, y, yerr))\n",
    "print('depth: ', np.exp(result.x[0]))\n",
    "print('duration: ', np.exp(result.x[1]))\n",
    "print('time: ', result.x[2])\n",
    "print('amp: ', np.exp(result.x[3]))\n",
    "print('ell: ', np.exp(result.x[4]))\n",
    "\n",
    "# visualize\n",
    "gp.interactive.setup_conditional_sampler_nonzero_mean_given_param(t, y, yerr, expsq_kernel, kernel_training_nonzero_mean,\n",
    "                                                                    result.x[0], result.x[1], result.x[2], result.x[3], result.x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 – Using GPs in Python\n",
    "\n",
    "In the previous section, we developed a fully-functional GP code that you could now use for research, but it might be better to use an existing software package that makes it easier to try out different models and some other niceties.\n",
    "There are packages in every programming language but, if you use Python as your programming language of choice, there are a few different software packages that you can use.\n",
    "Here are a few options:\n",
    "\n",
    "1. [george](http://dan.iel.fm/george) – Written (by this tutorial's original author!) with astronomy in mind. Good for general purpose GP modeling.\n",
    "2. [scikit-learn](http://scikit-learn.org/stable/modules/gaussian_process.html) – Well supported package with a big community. Can be used for astronomy projects with some work.\n",
    "3. [GPy](https://github.com/SheffieldML/GPy) – Very flexible with state-of-the-art inference algorithms designed by machine learning researchers.\n",
    "4. [PyMC](https://pymc-devs.github.io/pymc3/notebooks/GP-introduction.html) – Implementation in the context of Hamiltonian MCMC sampling.\n",
    "5. [Stan](http://mc-stan.org/) – Like PyMC, a Hamiltonian MCMC sampler with GP support.\n",
    "6. [celerite](http://celerite.readthedocs.io/) – Scalable GP likelihood calculations for 1D datasets (e.g. time series).\n",
    "7. etc. – I'm sure that there are others that I've missed!\n",
    "\n",
    "## 4 – Choosing a kernel\n",
    "\n",
    "Earlier in this tutorial, we chose to use the exponential-squared kernel function for the experiements.\n",
    "This was a good idea because it is *exactly* the kernel that I used to generate the data.\n",
    "That being said, in The Real World™, we never know what kernel to use and there are some other commonly used functions.\n",
    "One good simple example is the Matern-3/2 function\n",
    "\n",
    "$$\n",
    "k_{\\boldsymbol{\\alpha}}(r_{ij}) = a^2 \\, \\left[1+\\frac{\\sqrt{3}\\,|\\Delta x_{ij}|}{l}\\right]\\, \\exp \\left ( -\\frac{\\sqrt{3}\\,|\\Delta x_{ij}|}{l} \\right )\n",
    "$$\n",
    "\n",
    "Try implementing this function (or another one of your choice; see [Chapter 4](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) of [R&W](http://www.gaussianprocess.org/gpml/) for examples) and re-run the analysis with this kernel.\n",
    "There is no included unit test for this kernel (since this is an optional extension) bu‘t you might consider making some plots or writing your own tests to make sure that the function returns sensible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern32_kernel(alpha, dx):\n",
    "    \"\"\"\n",
    "    The Mater-3/2 kernel function. The difference matrix\n",
    "    can be an arbitrarily shaped numpy array so make sure that you\n",
    "    use functions like ``numpy.exp`` for exponentiation.\n",
    "    \n",
    "    :param alpha: ``(2,)`` The parameter vector ``(amp, ell)``.\n",
    "    :param dx: ``numpy.array`` The difference matrix. This can be\n",
    "        a numpy array with arbitrary shape.\n",
    "    \n",
    "    :returns K: The kernel matrix (should be the same shape as the\n",
    "        input ``dx``). \n",
    "    \n",
    "    \"\"\"\n",
    "    # Erase the following line and implement your kernel function\n",
    "    # there.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the conditional distribution given by this new choice of kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = gp.interactive.setup_conditional_sampler_nonzero_mean(t, y, yerr, matern32_kernel, kernel_training_nonzero_mean)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the MCMC sampling with this kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [np.log(np.abs(widget.kwargs[k])) for k in [\"depth\", \"duration\", \"amp\", \"ell\"]]\n",
    "pos = np.array(pos[:2] + [widget.kwargs[\"time\"]] + pos[2:]) \n",
    "pos = pos + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, gp_log_prob,\n",
    "                                args=(log_like, matern32_kernel, model, t, y, yerr))\n",
    "pos, _, _ = sampler.run_mcmc(pos, 200)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, 2000);\n",
    "\n",
    "gp.corner(sampler.flatchain, truths=np.append(gp.data.true_parameters, [None, None]),\n",
    "          labels=[\"ln(depth)\", \"ln(duration)\", \"transit time\", \"ln(amp)\", \"ln(ell)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the specific choices that you made, this may or may not look awesome.\n",
    "If not, try to debug the code (and make sure that you choose a good initial guess) until it does or, otherwise, come up with an argument for what is happening.\n",
    "\n",
    "As a general guideline, the choice of kernel often won't significantly affect your inferences about the parameters of the mean model.\n",
    "That being said, if you don't have a physical motivation for your kernel choice, it's useful to try some sort of formal model comparison.\n",
    "There is nothing special about kernel selection compared to other model selection problems so the usual techniques ([Bayes factors](https://en.wikipedia.org/wiki/Bayes_factor), [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29), [information criteria](https://en.wikipedia.org/wiki/Bayesian_information_criterion), etc.) are all applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 – Advanced: Outliers\n",
    "\n",
    "If you've ever tried to fit a model to real data, I'm sure that you've run into issues with outliers.\n",
    "In the context of white noise, there are [rigorous things that you can do](http://dan.iel.fm/posts/mixture-models/), but it gets a bit trickier with GPs.\n",
    "A detailed discussion of why mixture models are tricky to implement with GPs is beyond the scope of this tutorial, but the qualitative reason is that every data point \"cares\" about the identity – outlier or otherwise – of every other data point.\n",
    "There has been some work on alternatives to GPs that have heavier tails (e.g. [Student-*t* processes](https://arxiv.org/abs/1402.4306)), but the easiest (and most common) place to start is just our old friend sigma clipping.\n",
    "\n",
    "To check this out, let's simulate a dataset where 10% of the points are outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_out, y_out, yerr_out = gp.data.generate_dataset(outliers=75.0)\n",
    "plt.errorbar(t_out, y_out, yerr=yerr_out, fmt=\".k\")\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"flux [ppm]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's play around with this to see how the predictions change with these outliers.\n",
    "In particular, look at how the likelihood changes as you go to very short scale length `ell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = expsq_kernel\n",
    "widget = gp.interactive.setup_conditional_sampler_nonzero_mean(t_out, y_out, yerr_out, kernel, kernel_training_nonzero_mean)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from here, let's run MCMC without dealing with the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [np.log(np.abs(widget.kwargs[k])) for k in [\"depth\", \"duration\", \"amp\", \"ell\"]]\n",
    "pos = np.array(pos[:2] + [widget.kwargs[\"time\"]] + pos[2:]) \n",
    "pos = pos + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, gp_log_prob,\n",
    "                                args=(log_like, kernel, model, t_out, y_out, yerr_out))\n",
    "pos, _, _ = sampler.run_mcmc(pos, 200)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, 2000);\n",
    "\n",
    "gp.corner(sampler.flatchain, truths=np.append(gp.data.true_parameters, [None, None]),\n",
    "          labels=[\"ln(depth)\", \"ln(duration)\", \"transit time\", \"ln(amp)\", \"ln(ell)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look? Like garbage? This time, it's probably not a bug!\n",
    "\n",
    "To deal with this, you will want to implement a sigma clipping procedure.\n",
    "I'm not going to provide code for this, but the basic procedure is:\n",
    "\n",
    "1. Fit for the maximum likelihood (or posteriori) parameters using the un-clipped data.\n",
    "2. Implement Equations 2.22-2.24 from [R&W](http://www.gaussianprocess.org/gpml/chapters/) (or use an existing GP package) to compute the model prediction for the maximum likelihood parameters.\n",
    "3. Clip data points that are more than $K$-sigma away from the prediction (if you can work it through, it's best if you can let datapoints re-enter the fit).\n",
    "4. Return to Step 1 and repeat to convergence.\n",
    "\n",
    "This procedure should converge quickly and reliably to a dataset with fewer outliers.\n",
    "Some skeleton code is provided here, but it might be easier to use one of the Python packages mentioned above to implement this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initial guess\n",
    "pos = [np.log(np.abs(widget.kwargs[k])) for k in [\"depth\", \"duration\", \"amp\", \"ell\"]]\n",
    "pos = np.array(pos[:2] + [widget.kwargs[\"time\"]] + pos[2:])\n",
    "\n",
    "# Remember that you want to *minimuze* the function\n",
    "neg_log_prob = lambda *args: -gp_log_prob(*args)\n",
    "\n",
    "# Implement your sigma clipping procedure here...\n",
    "\n",
    "# This should be provided to the \"args\" argument in the minimize function\n",
    "# In each loop of clipping, update \"not_clipped\"\n",
    "not_clipped = np.ones(len(t_out), dtype=bool)\n",
    "args = (log_like, kernel, model, t_out[not_clipped], y_out[not_clipped], yerr_out[not_clipped])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you are satisfied with your procedure, try running the MCMC again and see if you get more reasonable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6 – Advanced: Computational cost and scaling of GPs\n",
    "\n",
    "One of the biggest problems that you'll run into when you use GPs in practice is the computational efficiency and scaling.\n",
    "To compute the likelihood of a GP model, you must invert (or, really, factorize) an $N \\times N$ matrix where $N$ is the number of datapoints.\n",
    "In general, the cost of this calcuation scales as the cube(!!!) of the size of the matrix.\n",
    "In other words, the runtime is proportional to $N^3$.\n",
    "This means that applying GP modeling to large datasets will be computationally intractable.\n",
    "\n",
    "To see this in practice, let’s take a moment to benchmark our code and see it scales with the size of the dataset.\n",
    "To test your implementation, run the following (this will take a minute or two to finish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sampler.flatchain[-1]\n",
    "t_bench, y_bench, yerr_bench = gp.data.generate_dataset(N=2**10)\n",
    "Ns = 2 ** np.arange(5, 11)\n",
    "times = np.empty(len(Ns))\n",
    "for i, N in enumerate(Ns):\n",
    "    result = %timeit -qo gp_log_prob(params, log_like, expsq_kernel, model, t_bench[:N], y_bench[:N], yerr_bench[:N])\n",
    "    times[i] = result.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot this and compare it to the theoretical scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ns, times, \".-k\", ms=9, label=\"GP\")\n",
    "plt.plot(Ns, 0.5 * times[-1] * (Ns / Ns[-1])**3, label=\"N cubed\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(Ns.min(), Ns.max())\n",
    "plt.ylim(0.8 * times.min(), 1.2 * times.max())\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical cost of the computation of the ln-likelihood should be $\\mathcal{O}(N^3)$ where $N$ is the number of datapoints.\n",
    "\n",
    "**Questions:**\n",
    "Is this what you get?\n",
    "Why or why not?\n",
    "How much time would it take to compute a model with 60,000-70,000 data points (not to mention the memory requirements!)?\n",
    "\n",
    "There is a discussion of methods for scaling GPs to larger datasets in [Chapter 8 of R&W](http://www.gaussianprocess.org/gpml/chapters/).\n",
    "The easiest place to start is by just sub-sampling your data.\n",
    "There are also a few existing software packages that can be used (e.g. [this](http://dan.iel.fm/george/current/user/solvers/#hodlr-solver) and [this](http://celerite.readthedocs.io)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
